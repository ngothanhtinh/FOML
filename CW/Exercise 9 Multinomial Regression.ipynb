{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "925a96bc",
   "metadata": {},
   "source": [
    "# Exercise 9: Multinomial Regression\n",
    "\n",
    "In multinomial regression, we have $K$ classes and inputs with $n$ features, i.e., $\\mathcal{X} = \\mathbb{R}^n$ and $\\mathcal{Y} = \\{ 1, \\ldots, K \\}$.\n",
    "\n",
    "The hypothesis, for class $k$, is the estimated probability $\\phi_k$ that $\\mathbf{x}$ was drawn from class $k$ using\n",
    "the linear transformation of the input followed by the softmax function:\n",
    "\n",
    "$$\\phi_k = \\frac{e^{\\theta_k^T \\mathbf{x}}}{\\sum_{j=1}^K e^{\\theta_j^T \\mathbf{x}}}.$$\n",
    "\n",
    "The cost function is the *cross entropy* loss function\n",
    "$$J(\\theta) = -\\sum_{i=1}^m \\log \\phi_{y^{(i)}}.$$\n",
    "\n",
    "The algorithm to minimize the cost $J(\\theta)$ is just gradient descent. The batch version of the gradient is\n",
    "$$ \\frac{\\partial J}{\\partial \\theta_{kl}} = - \\sum_{i=1}^m (\\delta(y^{(i)} = k) - \\phi_k)x_l^{(i)}.$$\n",
    "\n",
    "To demonstrate, we begin by reading the famous \"Iris\" dataset from [The UCI ML Dataset Repository](https://archive.ics.uci.edu/ml/datasets/iris) as a matrix $X$ of inputs and a vector $\\mathbf{y}$ of class labels in the set $\\{0, 1, 2\\}$ for classes 1, 2, and 3, which are different species of iris flowers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "51104ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import iris.data (CSV file)\n",
    "\n",
    "with open('/home/mdailey/work/PMDS FoML Exercise 1/iris.data', encoding=\"utf-8\") as f:\n",
    "    read_data = f.read()\n",
    "\n",
    "X = []\n",
    "Y_labels = []\n",
    "for line in read_data.splitlines():\n",
    "    data = line.split(',')\n",
    "    if len(data) < 5:\n",
    "        break\n",
    "    x = data[:4]\n",
    "    X.append(x)\n",
    "    y = data[4]\n",
    "    Y_labels.append(y)\n",
    "\n",
    "m = len(Y_labels)\n",
    "X = np.concatenate((np.ones((m, 1)), np.array(X, np.float32)), 1)\n",
    "Y_labels = np.array(Y_labels)\n",
    "y = np.zeros((m, 1))\n",
    "y[Y_labels == 'Iris-setosa'] = 0\n",
    "y[Y_labels == 'Iris-versicolor'] = 1\n",
    "y[Y_labels == 'Iris-virginica'] = 2\n",
    "\n",
    "K = 3\n",
    "\n",
    "# Inputs are now in X, and outputs are in y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a29a4b",
   "metadata": {},
   "source": [
    "Here is the hypothesis function outputting all $\\phi_k$ predictions for all $m$ values in matrix $X$, a cost\n",
    "function with gradient calculation, and a gradient descent loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e1c25725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: cost 1.098612\n",
      "Epoch 1000: cost 0.102589\n",
      "Epoch 2000: cost 0.082405\n",
      "Epoch 3000: cost 0.073794\n",
      "Epoch 4000: cost 0.068795\n",
      "Epoch 5000: cost 0.065431\n",
      "Epoch 6000: cost 0.062962\n",
      "Epoch 7000: cost 0.061044\n",
      "Epoch 8000: cost 0.059493\n",
      "Epoch 9000: cost 0.058202\n",
      "Epoch 10000: cost 0.057103\n",
      "Epoch 11000: cost 0.056151\n",
      "Epoch 12000: cost 0.055316\n",
      "Epoch 13000: cost 0.054573\n",
      "Epoch 14000: cost 0.053907\n",
      "Epoch 15000: cost 0.053304\n",
      "Epoch 16000: cost 0.052755\n",
      "Epoch 17000: cost 0.052253\n",
      "Epoch 18000: cost 0.051790\n",
      "Epoch 19000: cost 0.051362\n",
      "Epoch 20000: cost 0.050965\n",
      "Epoch 21000: cost 0.050594\n",
      "Epoch 22000: cost 0.050248\n",
      "Epoch 23000: cost 0.049923\n",
      "Epoch 24000: cost 0.049617\n",
      "Epoch 25000: cost 0.049329\n",
      "Epoch 26000: cost 0.049057\n",
      "Epoch 27000: cost 0.048799\n",
      "Epoch 28000: cost 0.048554\n",
      "Epoch 29000: cost 0.048322\n",
      "Epoch 30000: cost 0.048101\n",
      "Epoch 31000: cost 0.047891\n",
      "Epoch 32000: cost 0.047690\n",
      "Epoch 33000: cost 0.047497\n",
      "Epoch 34000: cost 0.047314\n",
      "Epoch 35000: cost 0.047138\n",
      "Epoch 36000: cost 0.046969\n",
      "Epoch 37000: cost 0.046807\n",
      "Epoch 38000: cost 0.046651\n",
      "Epoch 39000: cost 0.046501\n",
      "Epoch 40000: cost 0.046357\n",
      "Epoch 41000: cost 0.046218\n",
      "Epoch 42000: cost 0.046084\n",
      "Epoch 43000: cost 0.045955\n",
      "Epoch 44000: cost 0.045830\n",
      "Epoch 45000: cost 0.045710\n",
      "Epoch 46000: cost 0.045593\n",
      "Epoch 47000: cost 0.045481\n",
      "Epoch 48000: cost 0.045372\n",
      "Epoch 49000: cost 0.045266\n",
      "Epoch 50000: cost 0.045164\n",
      "Epoch 51000: cost 0.045065\n",
      "Epoch 52000: cost 0.044968\n",
      "Epoch 53000: cost 0.044875\n",
      "Epoch 54000: cost 0.044784\n",
      "Epoch 55000: cost 0.044696\n",
      "Epoch 56000: cost 0.044611\n",
      "Epoch 57000: cost 0.044527\n",
      "Epoch 58000: cost 0.044447\n",
      "Epoch 59000: cost 0.044368\n",
      "Epoch 60000: cost 0.044291\n",
      "Epoch 61000: cost 0.044217\n",
      "Epoch 62000: cost 0.044144\n",
      "Epoch 63000: cost 0.044073\n",
      "Epoch 64000: cost 0.044004\n",
      "Epoch 65000: cost 0.043937\n",
      "Epoch 66000: cost 0.043871\n",
      "Epoch 67000: cost 0.043807\n",
      "Epoch 68000: cost 0.043745\n",
      "Epoch 69000: cost 0.043684\n",
      "Epoch 70000: cost 0.043624\n",
      "Epoch 71000: cost 0.043566\n",
      "Epoch 72000: cost 0.043509\n",
      "Epoch 73000: cost 0.043454\n",
      "Epoch 74000: cost 0.043400\n",
      "Epoch 75000: cost 0.043347\n",
      "Epoch 76000: cost 0.043295\n",
      "Epoch 77000: cost 0.043244\n",
      "Epoch 78000: cost 0.043194\n",
      "Epoch 79000: cost 0.043146\n",
      "Epoch 80000: cost 0.043098\n",
      "Epoch 81000: cost 0.043052\n",
      "Epoch 82000: cost 0.043006\n",
      "Epoch 83000: cost 0.042961\n",
      "Epoch 84000: cost 0.042918\n",
      "Epoch 85000: cost 0.042875\n",
      "Epoch 86000: cost 0.042833\n",
      "Epoch 87000: cost 0.042792\n",
      "Epoch 88000: cost 0.042751\n",
      "Epoch 89000: cost 0.042712\n",
      "Epoch 90000: cost 0.042673\n",
      "Epoch 91000: cost 0.042635\n",
      "Epoch 92000: cost 0.042598\n",
      "Epoch 93000: cost 0.042561\n",
      "Epoch 94000: cost 0.042525\n",
      "Epoch 95000: cost 0.042490\n",
      "Epoch 96000: cost 0.042455\n",
      "Epoch 97000: cost 0.042421\n",
      "Epoch 98000: cost 0.042388\n",
      "Epoch 99000: cost 0.042355\n"
     ]
    }
   ],
   "source": [
    "theta = np.zeros((K, X.shape[1]))\n",
    "\n",
    "def hypothesis(X, theta):\n",
    "    K = theta.shape[0]\n",
    "    m = X.shape[0]\n",
    "    a = np.zeros((m, K))\n",
    "    for k in range(K):\n",
    "        a[:,k:k+1] = X @ theta[k:k+1,:].T\n",
    "    e_a = np.exp(a)\n",
    "    return e_a / e_a.sum(axis=1, keepdims=True)\n",
    "\n",
    "def onehot(y, K):\n",
    "    Y = np.zeros((y.shape[0], K))\n",
    "    for k in range(K):\n",
    "        Y[y.reshape(-1)==k,k] = 1\n",
    "    return Y\n",
    "    \n",
    "def cost(X, y, theta):\n",
    "    K = theta.shape[0]\n",
    "    m = X.shape[0]\n",
    "    yhat = hypothesis(X, theta)\n",
    "    # Form the matrix delta(y=k)\n",
    "    Y = onehot(y, K)\n",
    "    J = 0\n",
    "    for k in range(K):\n",
    "        J = J - np.log(yhat[y.reshape(-1)==k,k]).sum()\n",
    "    J = J / m\n",
    "    # Get the gradient matrix for theta\n",
    "    grad_J = (yhat - Y).T @ X\n",
    "    return J, grad_J\n",
    "\n",
    "alpha = 0.001\n",
    "for epoch in range(100000):\n",
    "    J, grad_J = cost(X, y, theta)\n",
    "    if epoch % 1000 == 0:\n",
    "        print('Epoch %d: cost %f' % (epoch, J))\n",
    "    theta = theta - alpha * grad_J\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e62caf",
   "metadata": {},
   "source": [
    "Next we calculate the (training set) accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d9f32cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.986667\n"
     ]
    }
   ],
   "source": [
    "preds = hypothesis(X, theta)\n",
    "pred_classes = np.argmax(preds, axis=1)\n",
    "print('Training set accuracy: %f' % ((pred_classes==y.reshape(-1)).sum()/m))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb6fa91",
   "metadata": {},
   "source": [
    "Finally, let's understand the softmax function in a bit more detail. The softmax refers to the idea that we\n",
    "would like to find the maximum output of a linear transformation of the inputs, but keep the information\n",
    "about the relative contribution of the $K$ different outputs.\n",
    "\n",
    "The softmax function, for a vector of activations $\\mathbf{a}$, is defined as\n",
    "$$\\phi_k = \\frac{e^{a_k}}{\\sum_{j=1}^K e^{a_j}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "af7a2715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02778834 0.33853132 0.07553655 0.55814379]\n",
      "[0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "linear_activations = np.array([ -2, 0.5, -1, 1 ])\n",
    "\n",
    "def softmax(a):\n",
    "    return np.exp(a) / np.exp(a).sum()\n",
    "\n",
    "def hardmax(a):\n",
    "    phi = np.zeros(len(a))\n",
    "    phi[a.argmax()] = 1\n",
    "    return phi\n",
    "\n",
    "print(softmax(linear_activations))\n",
    "print(hardmax(linear_activations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f36bf78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
